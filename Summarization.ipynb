{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"dataframe.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempDf = df[['asin','Summary_Clean']]\n",
    "newdf = tempDf.groupby(['asin'])['Summary_Clean'].apply(lambda x: \". \".join(x.astype(str))).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import *\n",
    "import logging\n",
    "import torch\n",
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "from typing import List\n",
    "\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "class ModelSelector(object):\n",
    "    MODELS = {\n",
    "        'bert-base-uncased': (BertModel, BertTokenizer),\n",
    "        'bert-large-uncased': (BertModel, BertTokenizer)\n",
    "    }\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str,\n",
    "        custom_model: PreTrainedModel=None,\n",
    "        custom_tokenizer: PreTrainedTokenizer=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param model: Model is the string path for the bert weights. If given a keyword, the s3 path will be used\n",
    "        :param custom_model: This is optional if a custom bert model is used\n",
    "        :param custom_tokenizer: Place to use custom tokenizer\n",
    "        \"\"\"\n",
    "\n",
    "        base_model, base_tokenizer = self.MODELS.get(model, (None, None))\n",
    "\n",
    "        if custom_model:\n",
    "            self.model = custom_model\n",
    "        else:\n",
    "            self.model = base_model.from_pretrained(model, output_hidden_states=True)\n",
    "\n",
    "        if custom_tokenizer:\n",
    "            self.tokenizer = custom_tokenizer\n",
    "        else:\n",
    "            self.tokenizer = base_tokenizer.from_pretrained(model)\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "    def inputtokenizor(self, text: str) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        Tokenizes the text input.\n",
    "        :param text: Text to tokenize\n",
    "        :return: Returns a torch tensor\n",
    "        \"\"\"\n",
    "        tokenized_text = self.tokenizer.tokenize(text)\n",
    "        indexed_tokens = self.tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "        return torch.tensor([indexed_tokens])\n",
    "\n",
    "    def embdeddingsextractor(\n",
    "        self,\n",
    "        text: str,\n",
    "        hidden: int=-2,\n",
    "        squeeze: bool=False,\n",
    "        reduce_option: str ='mean'\n",
    "    ) -> ndarray:\n",
    "\n",
    "        tokens_tensor = self.inputtokenizor(text)\n",
    "        pooled, hidden_states = self.model(tokens_tensor)[-2:]\n",
    "\n",
    "        if -1 > hidden > -12:\n",
    "\n",
    "            if reduce_option == 'max':\n",
    "                pooled = hidden_states[hidden].max(dim=1)[0]\n",
    "\n",
    "            elif reduce_option == 'median':\n",
    "                pooled = hidden_states[hidden].median(dim=1)[0]\n",
    "\n",
    "            else:\n",
    "                pooled = hidden_states[hidden].mean(dim=1)\n",
    "\n",
    "        if squeeze:\n",
    "            return pooled.detach().numpy().squeeze()\n",
    "        #print(pooled)\n",
    "        return pooled\n",
    "\n",
    "    def generateMat(\n",
    "        self,\n",
    "        content: List[str],\n",
    "        hidden: int=-2,\n",
    "        reduce_option: str = 'mean'\n",
    "    ) -> ndarray:\n",
    "\n",
    "        return np.asarray([\n",
    "            np.squeeze(self.embdeddingsextractor(t, hidden=hidden, reduce_option=reduce_option).data.numpy())\n",
    "            for t in content\n",
    "        ])\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        content: List[str],\n",
    "        hidden: int= -2,\n",
    "        reduce_option: str = 'mean'\n",
    "    ) -> ndarray:\n",
    "        return self.generateMat(content, hidden, reduce_option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.decomposition import PCA\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class Extractor(object):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        features: ndarray,\n",
    "        algorithm: str = 'kmeans',\n",
    "        pca_k: int = None,\n",
    "        random_state: int = 12345\n",
    "    ):\n",
    "\n",
    "        if pca_k:\n",
    "            self.features = PCA(n_components=pca_k).fit_transform(features)\n",
    "        else:\n",
    "            self.features = features\n",
    "\n",
    "        self.algorithm = algorithm\n",
    "        self.pca_k = pca_k\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def get_model(self, k: int):\n",
    "        if self.algorithm == 'gmm':\n",
    "            return GaussianMixture(n_components=k, random_state=self.random_state)\n",
    "        return KMeans(n_clusters=k, random_state=self.random_state)\n",
    "\n",
    "    def get_clustercentroids(self, model):\n",
    "        if self.algorithm == 'gmm':\n",
    "            return model.means_\n",
    "        return model.cluster_centers_\n",
    "\n",
    "    def find_closest(self, centroids: np.ndarray):\n",
    "        centroid_min = 1e10\n",
    "        cur_arg = -1\n",
    "        args = {}\n",
    "        used_idx = []\n",
    "\n",
    "        for j, centroid in enumerate(centroids):\n",
    "\n",
    "            for i, feature in enumerate(self.features):\n",
    "                value = np.linalg.norm(feature - centroid)\n",
    "\n",
    "                if value < centroid_min and i not in used_idx:\n",
    "                    cur_arg = i\n",
    "                    centroid_min = value\n",
    "\n",
    "            used_idx.append(cur_arg)\n",
    "            args[j] = cur_arg\n",
    "            centroid_min = 1e10\n",
    "            cur_arg = -1\n",
    "\n",
    "        return args\n",
    "\n",
    "    def cluster(self, ratio: float = 0.1) -> List[int]:\n",
    "        k = 1 if ratio * len(self.features) < 1 else int(len(self.features) * ratio)\n",
    "        model = self.get_model(k).fit(self.features)\n",
    "        centroids = self.get_clustercentroids(model)\n",
    "        cluster_args = self.find_closest(centroids)\n",
    "        sorted_values = sorted(cluster_args.values())\n",
    "        #print sorted_values\n",
    "        return sorted_values\n",
    "\n",
    "    def __call__(self, ratio: float = 0.1) -> List[int]:\n",
    "        return self.cluster(ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import List\n",
    "\n",
    "from abc import abstractmethod\n",
    "import neuralcoref\n",
    "from spacy.lang.en import English\n",
    "import numpy as np\n",
    "from transformers import PreTrainedModel, PreTrainedTokenizer\n",
    "\n",
    "\n",
    "class ModelCreator(object):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model='bert-large-uncased',\n",
    "        custom_model: PreTrainedModel = None,\n",
    "        custom_tokenizer: PreTrainedTokenizer = None,\n",
    "        hidden: int=-2,\n",
    "        reduce_option: str = 'mean',\n",
    "        greedyness: float=0.45,\n",
    "        language=English,\n",
    "        random_state: int = 12345\n",
    "    ):\n",
    "        np.random.seed(random_state)\n",
    "        self.model = ModelSelector(model, custom_model, custom_tokenizer)\n",
    "        self.hidden = hidden\n",
    "        self.reduce_option = reduce_option\n",
    "        self.nlp = language()\n",
    "        self.random_state = random_state\n",
    "        self.nlp.add_pipe(self.nlp.create_pipe('sentencizer'))\n",
    "        neuralcoref.add_to_pipe(self.nlp, greedyness=greedyness)\n",
    "\n",
    "    def process_content_sentences(self, body: str, min_length=40, max_length=600) -> List[str]:\n",
    "        doc = self.nlp(body)._.coref_resolved\n",
    "        doc = self.nlp(doc)\n",
    "        return [c.string.strip() for c in doc.sents if max_length > len(c.string.strip()) > min_length]\n",
    "\n",
    "    @abstractmethod\n",
    "    def run_clusters(self, content: List[str], ratio=0.2, algorithm='kmeans', use_first: bool=True) -> List[str]:\n",
    "        raise NotImplementedError(\"Must Implement run_clusters\")\n",
    "\n",
    "    def run(\n",
    "        self,\n",
    "        body: str,\n",
    "        ratio: float=0.2,\n",
    "        min_length: int=40,\n",
    "        max_length: int=600,\n",
    "        use_first: bool=True,\n",
    "        algorithm='kmeans'\n",
    "    ) -> str:\n",
    "        sentences = self.process_content_sentences(body, min_length, max_length)\n",
    "\n",
    "        if sentences:\n",
    "            sentences = self.run_clusters(sentences, ratio, algorithm, use_first)\n",
    "\n",
    "        return ' '.join(sentences)\n",
    "\n",
    "    def __call__(self, body: str, ratio: float=0.2, min_length: int=40, max_length: int=600,\n",
    "                 use_first: bool=True, algorithm='kmeans') -> str:\n",
    "        return self.run(body, ratio, min_length, max_length)\n",
    "\n",
    "\n",
    "class SelModel(ModelCreator):\n",
    "    \"\"\"\n",
    "    Deprecated for naming sake.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model='bert-large-uncased',\n",
    "        custom_model: PreTrainedModel = None,\n",
    "        custom_tokenizer: PreTrainedTokenizer = None,\n",
    "        hidden: int=-2,\n",
    "        reduce_option: str = 'mean',\n",
    "        greedyness: float=0.45,\n",
    "        language=English,\n",
    "        random_state: int=12345\n",
    "    ):\n",
    "        super(SelModel, self).__init__(model, custom_model, custom_tokenizer, hidden, reduce_option,\n",
    "                                          greedyness, language=language, random_state=random_state)\n",
    "\n",
    "    def run_clusters(self, content: List[str], ratio=0.2, algorithm='kmeans', use_first: bool= True) -> List[str]:\n",
    "        hidden = self.model(content, self.hidden, self.reduce_option)\n",
    "        hidden_args = Extractor(hidden, algorithm, random_state=self.random_state).cluster(ratio)\n",
    "        \n",
    "        if use_first:\n",
    "            if hidden_args[0] != 0:\n",
    "                hidden_args.insert(0,0)\n",
    "\n",
    "        return [content[j] for j in hidden_args]\n",
    "\n",
    "\n",
    "class SummaryExtractor(SelModel):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model='bert-large-uncased',\n",
    "        custom_model: PreTrainedModel = None,\n",
    "        custom_tokenizer: PreTrainedTokenizer = None,\n",
    "        hidden: int=-2,\n",
    "        reduce_option: str = 'mean',\n",
    "        greedyness: float=0.45,\n",
    "        language=English,\n",
    "        random_state: int=12345\n",
    "    ):\n",
    "        super(SummaryExtractor, self).__init__(model, custom_model, custom_tokenizer, hidden, reduce_option, greedyness, language, random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1213 17:29:25.716083  5160 configuration_utils.py:151] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-config.json from cache at C:\\Users\\SAITEJA-WORKMACHINE\\.cache\\torch\\transformers\\6dfaed860471b03ab5b9acb6153bea82b6632fb9bbe514d3fff050fe1319ee6d.4c88e2dec8f8b017f319f6db2b157fee632c0860d9422e4851bd0d6999f9ce38\n",
      "I1213 17:29:25.722625  5160 configuration_utils.py:168] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I1213 17:29:25.960902  5160 modeling_utils.py:337] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-pytorch_model.bin from cache at C:\\Users\\SAITEJA-WORKMACHINE\\.cache\\torch\\transformers\\54da47087cc86ce75324e4dc9bbb5f66c6e83a7c6bd23baea8b489acc8d09aa4.4d5343a4b979c4beeaadef17a0453d1bb183dd9b084f58b84c7cc781df343ae6\n",
      "I1213 17:29:48.531719  5160 tokenization_utils.py:374] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-vocab.txt from cache at C:\\Users\\SAITEJA-WORKMACHINE\\.cache\\torch\\transformers\\9b3c03a36e83b13d5ba95ac965c9f9074a99e14340c523ab405703179e79fc46.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "summaries=[]\n",
    "model = SummaryExtractor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7997, -0.2623,  0.1020,  ..., -0.2801, -0.4582,  0.0678]],\n",
      "       grad_fn=<MeanBackward0>)\n",
      "tensor([[-0.7483, -0.6802, -0.4951,  ...,  0.1665,  0.7793,  0.2888]],\n",
      "       grad_fn=<MeanBackward0>)\n",
      "tensor([[-1.3995, -0.9091, -0.4589,  ...,  0.3817,  1.1973,  0.1835]],\n",
      "       grad_fn=<MeanBackward0>)\n",
      "tensor([[-0.8755, -0.3848,  0.1581,  ...,  0.3399,  1.0221,  0.2653]],\n",
      "       grad_fn=<MeanBackward0>)\n",
      "tensor([[-0.1569, -0.4581, -0.2316,  ..., -0.1576, -0.2283, -0.0215]],\n",
      "       grad_fn=<MeanBackward0>)\n",
      "tensor([[-0.4123, -0.8063, -0.4375,  ...,  0.0957,  0.7057,  0.3471]],\n",
      "       grad_fn=<MeanBackward0>)\n",
      "0 awesome game if it did not crash frequently. an overlooked gem in the forza gt treasure trove.\n",
      "1 \n",
      "tensor([[-0.4901, -0.6434, -0.5977,  ..., -0.1877,  0.7327,  0.5372]],\n",
      "       grad_fn=<MeanBackward0>)\n",
      "2 the most hated videogame of all time and greatest betrayal of a fanbase in gaming history.\n",
      "3 \n",
      "tensor([[-1.1053, -0.4314, -0.8595,  ...,  0.5209,  0.5787,  0.6144]],\n",
      "       grad_fn=<MeanBackward0>)\n",
      "4 an unfortunate disappointment campaign review only.\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    \n",
    "    body = newdf['Summary_Clean'][i]\n",
    "    temp = model(body)\n",
    "    temp = temp.strip()\n",
    "    print(i,temp)\n",
    "#     print('l',len(temp))\n",
    "    summaries.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pay to unlock content i don t think so. good rally game. wrong key. awesome game if it did not crash frequently. dirt. good racing game terrible windows live requirement. a step up from dirt and that is terrific. crash is correct name aka microsoft. a great game ruined by microsoft s account management system. couldn t get this one to work. best in the series. a stars winner. cars. it might have been a good game but i never found out because the. don t waste your money. not as good as dirt. an overlooked gem in the forza gt treasure trove. better than dirt except for. colin mcrae crash. the first one was much better. this games is amazing. abysmal support from codemasters. games for windows live. fun. best graphics of any game so far'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#newdf['Summary_Clean'][2]\n",
    "newdf['Summary_Clean'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Four score and seven years ago our fathers brought forth on this continent, a new nation, conceived in Liberty, \\n            and dedicated to the proposition that all men are created equal. It is altogether fitting and proper that we should do this. It is rather for us to be here dedicated to the great task remaining before us -- that from these honored dead we take increased devotion to that cause for which they gave the last full measure of devotion -- that we here highly resolve that these dead shall not have died in vain \\n            -- that this nation, under God, shall have a new birth of freedom \\n            -- and that government of the people, by the people, for the people, shall not perish from the earth.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gettys = '''Four score and seven years ago our fathers brought forth on this continent, a new nation, conceived in Liberty, \n",
    "            and dedicated to the proposition that all men are created equal.\n",
    "\n",
    "            Now we are engaged in a great civil war, testing whether that nation, or any nation so conceived and so dedicated, can long endure. \n",
    "            We are met on a great battle-field of that war. \n",
    "            We have come to dedicate a portion of that field, as a final resting place for those who here gave their lives that that nation might live. \n",
    "            It is altogether fitting and proper that we should do this.\n",
    "\n",
    "            But, in a larger sense, we can not dedicate -- we can not consecrate -- we can not hallow -- this ground. \n",
    "            The brave men, living and dead, who struggled here, have consecrated it, far above our poor power to add or detract.\n",
    "            The world will little note, nor long remember what we say here, but it can never forget what they did here. \n",
    "            It is for us the living, rather, to be dedicated here to the unfinished work which they who fought here have thus far so nobly advanced. \n",
    "            It is rather for us to be here dedicated to the great task remaining before us -- that from these honored dead we take increased devotion to that cause for which they gave the last full measure of devotion -- that we here highly resolve that these dead shall not have died in vain \n",
    "            -- that this nation, under God, shall have a new birth of freedom \n",
    "            -- and that government of the people, by the people, for the people, shall not perish from the earth.'''\n",
    "model(gettys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ad sales boost Time Warner profit\\n\\nQuarterly profits at US media giant TimeWarner jumped 76% to $1.13bn (£600m) for the three months to December, from $639m year-earlier. The firm, which is now one of the biggest investors in Google, benefited from sales of high-speed internet connections and higher advert sales. It lost 464,000 subscribers in the fourth quarter profits were lower than in the preceding three quarters. But its film division saw profits slump 27% to $284m, helped by box-office flops Alexander and Catwoman, a sharp contrast to year-earlier, when the third and final film in the Lord of the Rings trilogy boosted results. Our financial performance was strong, meeting or exceeding all of our full-year objectives and greatly enhancing our flexibility,\" chairman and chief executive Richard Parsons said.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gettys = '''Ad sales boost Time Warner profit\n",
    "\n",
    "Quarterly profits at US media giant TimeWarner jumped 76% to $1.13bn (£600m) for the three months to December, from $639m year-earlier.\n",
    "\n",
    "The firm, which is now one of the biggest investors in Google, benefited from sales of high-speed internet connections and higher advert sales. TimeWarner said fourth quarter sales rose 2% to $11.1bn from $10.9bn. Its profits were buoyed by one-off gains which offset a profit dip at Warner Bros, and less users for AOL.\n",
    "\n",
    "Time Warner said on Friday that it now owns 8% of search-engine Google. But its own internet business, AOL, had has mixed fortunes. It lost 464,000 subscribers in the fourth quarter profits were lower than in the preceding three quarters. However, the company said AOL's underlying profit before exceptional items rose 8% on the back of stronger internet advertising revenues. It hopes to increase subscribers by offering the online service free to TimeWarner internet customers and will try to sign up AOL's existing customers for high-speed broadband. TimeWarner also has to restate 2000 and 2003 results following a probe by the US Securities Exchange Commission (SEC), which is close to concluding.\n",
    "\n",
    "Time Warner's fourth quarter profits were slightly better than analysts' expectations. But its film division saw profits slump 27% to $284m, helped by box-office flops Alexander and Catwoman, a sharp contrast to year-earlier, when the third and final film in the Lord of the Rings trilogy boosted results. For the full-year, TimeWarner posted a profit of $3.36bn, up 27% from its 2003 performance, while revenues grew 6.4% to $42.09bn. \"Our financial performance was strong, meeting or exceeding all of our full-year objectives and greatly enhancing our flexibility,\" chairman and chief executive Richard Parsons said. For 2005, TimeWarner is projecting operating earnings growth of around 5%, and also expects higher revenue and wider profit margins.\n",
    "\n",
    "TimeWarner is to restate its accounts as part of efforts to resolve an inquiry into AOL by US market regulators. It has already offered to pay $300m to settle charges, in a deal that is under review by the SEC. The company said it was unable to estimate the amount it needed to set aside for legal reserves, which it previously set at $500m. It intends to adjust the way it accounts for a deal with German music publisher Bertelsmann's purchase of a stake in AOL Europe, which it had reported as advertising revenue. It will now book the sale of its stake in AOL Europe as a loss on the value of that stake.\n",
    "'''\n",
    "model(gettys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original =  pay to unlock content i don t think so. good rally game. wrong key. awesome game if it did not crash frequently. dirt. good racing game terrible windows live requirement. a step up from dirt and that is terrific. crash is correct name aka microsoft. a great game ruined by microsoft s account management system. couldn t get this one to work. best in the series. a stars winner. cars. it might have been a good game but i never found out because the. don t waste your money. not as good as dirt. an overlooked gem in the forza gt treasure trove. better than dirt except for. colin mcrae crash. the first one was much better. this games is amazing. abysmal support from codemasters. games for windows live. fun. best graphics of any game so far\n",
      "original =  works good. yet another great expansion. usb microphone. works with rb on the. too fun for words. limited compatability. does not work for wii\n",
      "original =  epic zelda title. tremendous game. the most hated videogame of all time and greatest betrayal of a fanbase in gaming history. amazing. a sturdy rival of ocarina of time. i regret buying this. a mix to a familiar formula. awesome game\n",
      "original =  i want to give it four stars but. for the price its great. exactly what was needed. fuzzy picture. works grate. just as good as the oem product\n",
      "original =  not as good as the first one. boring. an unfortunate disappointment campaign review only. fun game but mostly arcade. this is not as bad as others have said\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    body = newdf['Summary_Clean'][i]\n",
    "    print(\"original = \",body)\n",
    "    summaries.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'an amazing fighter out of. lets smash something then be really bored. nan. an amazing game for its time. my god. very fun still to this day. great personal combat game. super smash bros is da bomb. one of the best and must have games for the n console. great game but needs more replay it s hard not like though. if you own a nintendo then you should own this game. a classic. i ll kick you stuffed boot ey. fun for all ages. excellent. comical and chaotic. smash down your friends. your favorite nintendo chatacters put in awesome game. came in great working order. a great fighting game with your favorite nintendo characters. the clash of the nintendo titans. best n game. classic n title. smash em other fighter games this game rox. it ll get boring after about vs mode matches. good game for all. very fun game. ok to rent not so great to buy. almost as good as the sequel still an excellent game. this game rocks. my kids pay this game more than any other. tied for second best in the series. super smash brothers is super awesome brothers. awesome. the game works. finally a good fighting game on the n. time to kick some butt get ready to smash nintendo style. the least amount of replay value in an n game ever. start of greatness. the one that started it all. smash bros is one big smash. powerful characters. okey for a while. smashing. the best n fight game ever. why can t we be friends. the best fighting game ever. the best fighting game. an n classic. one of the most addicting games ever made. a no brainer. great multiplayer or single player action. awesome game for n. i love this game. a game that will definately show you a good time. a very fun game. the best nintendo game on the planet a must buy. working perfecrly. battle. super smash bros n. super smash bros a very good game. good multiplayer game. super smash brothers a work in progress. sweet. a smashing game. a great fighting game for n. fun strategic and fast. its a melee all right i really did not cling to it. old school'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdf['Summary_Clean'][86]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lets smash something then be really bored. one of the best and must have games for the n console. if you own a nintendo then you should own this game. smash em other fighter games this game rox. my kids pay this game more than any other.'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(newdf['Summary_Clean'][86])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
